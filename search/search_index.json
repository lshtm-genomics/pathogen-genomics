{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pathogen genomics","text":"<p>Please click the links below for practical materials:</p> <ul> <li>Microbiome</li> <li>TB genomics</li> <li>Oxford nanopore</li> </ul>"},{"location":"microbiome/","title":"Microbiomes Practical","text":"<p>Bacterial vaginosis (BV) is a dysbiotic condition caused by excessive growth of certain bacteria replacing the regular vaginal microbiome. Common symptoms include increased discharge, burning with urination, and itching. BV increases the risk of infection by a number of sexually transmitted infections including HIV/AIDS as well as the risk of early delivery when pregnant. The changed composition of the microbiome leads to a higher pH and a hundred to thousand-fold increase in the total number of bacteria present.</p>"},{"location":"microbiome/#getting-the-data","title":"Getting the data","text":"<p>For this practical we are considering 12 samples of vaginal swab that were taken at a polyclinic by a GP in a setting of high transmission of HIV. DNA was extracted from the swabs and amplified using primers specific for the first two hypervariable regions (V1 and V2) of the 16S rRNA gene (27F and 338R). These samples were then sequenced with MiSeq Illumina producing paired end data of 300 bp length per read. The 12 pairs of files generated are found in the data/microbiome/fastq/ directory. The patients\u2019 phenotype was determined by the doctors at the time of sample collection with the following results:</p> Sample BV pH BB_1 no 4.4 BB_2 no 3.6 BB_3 yes 5.5 BB_4 no 5.3 BB_5 yes 5.6 BB_6 yes 5.3 BB_7 yes 4.7 BB_8 no 4.4 BB_9 no 4.4 BB_10 no 3.6 BB_11 yes 4.7 BB_12 yes 5"},{"location":"microbiome/#analysing-the-microbiome-samples-with-qiime2","title":"Analysing the microbiome samples with QIIME2","text":"<p>Out of all tookits aiming at the unification of the analysis of microbiome data, QIIME (pronounced \"chime\") and its successor QIIME2 have grown the largest user base in recent years (mostly due to the ease of use and comprehensive online documentation). QIIME2 wraps an extensive suite of third party tools (covering most of the \"standard\" microbiome pipeline from preprocessing and filtering of raw sequencing reads to statistical tests on diversity metrics and analyses on differential abundance of single taxa) into a single command line interface. In addition, it also provides a GUI as well as a python API for both less and more technically inclined users. We will stay on the middle ground by using the CLI today. One idiosyncrasy of QIIME2 is the use of so-called \"artefacts\". These are zip-archives with a special file extension (.qza for data artefacts and .qzv for visualisation artefacts) that hold bulk data in addition to unique IDs and provenance metadata, which describe all steps that lead to the creation of that particular artefact. This has the advantage that for every intermediate or final result of qiime it is perfectly clear how it was generated from start to finish. There is a small downside, though, since we have to import our data into the QIIME2 format prior to running any analyses. However, before we do that, let's have a look at the quality of our reads (qiime also provides functionality for sequencing data quality control, but it is not as detailed as the output of some dedicated tools like FastQC). </p>"},{"location":"microbiome/#quality-control","title":"Quality control","text":"<p>After activating the conda environment for this practical with <code>conda activate microbiome</code>, go into the module directory with <code>cd data/microbiome</code> and have a look at its contents with <code>ls</code>. There should be a directory with the 16S sequencing data (in fastq), a 16S database (in db), and a CSV file with our metadata. Let's check if our reads are there with <code>ls fastq</code>. We can also have a look at the filesizes with <code>du -sh fastq/* | sort -h</code> (it can't hurt to get a feeling for these things). </p> <pre><code>mkdir fastqc_reports\n\nfastqc -o fastqc_reports -q -t 1 fastq/*\n</code></pre> <p>Info</p> <p>The -t flag tells fastqc the number of threads to use. If you have more CPUs available, adjust this number accordingly. </p> <p>This should produce a FastQC report for each fastq file and put them all into fastqc_reports (run <code>ls fastqc_reports</code> to double check). Going through 24 FastQC reports (two per sample; one for the forward and one for the reverse reads) manually would be quite tedious. Thankfully, multiqc can combine them for us! Let's create a new directory for it to write the results into and run it. Before you do that, install the relevant package with <code>mamba install multiqc</code> and <code>pip install rich_click</code>.</p> <pre><code>mkdir fastqc_combined\n\nmultiqc -o fastqc_combined fastqc_reports\n</code></pre> <p>With <code>ls fastqc_combined</code> you can see that an HTML file, which we can view in a browser, has been created.</p> <p>Scroll through the report and make note of the sequence counts barplots and the quality histograms. </p> <p></p> <p></p> <p>In the counts plot we can see that two samples (BB_3 and BB_8) have significantly fewer reads than the others. This looks like something went wrong during library preparation for these two samples and we should exclude them from further analysis. Also note that the reads of the other samples have decent quality up until ~200 bp length. We will need this information later. </p> <p>Question</p> <p>The difference in file sizes of the sequencing files between BB_3 / BB_8 and the other samples was not as drastic as the difference in actual read counts. Can you think of a reason why that might be? Hint: What does the file extension .gz mean? </p> <p>For this dataset, the primers have already been trimmed from the reads and the FastQC output showed us that there are no adapters that would need removal. Also, quality-based trimming is discouraged when using DADA2 (an important step in our later analyses). Therefore, no further pre-processing is needed and we can transform the data into the qiime format. </p>"},{"location":"microbiome/#import-into-qiime","title":"Import into qiime","text":"<p>In order to do this, qiime needs a tab-separated file with the sample IDs and the absolute paths to the forward and reverse reads. There are many ways to create such a file (and if you have just a few samples you can simply type it by hand). We will use the opportunity to string a few handy command line utilities together that we have not seen so far. First, let's write the header line of the import-list to a new file: </p> <pre><code>printf \\\n    \"sample-id\\tforward-absolute-filepath\\treverse-absolute-filepath\\n\" \\\n    &gt; fastq_abs_paths\n</code></pre> <p>Info</p> <p>We use backslashs here to break this command into multiple lines. </p> <p>Then, we append the lines corresponding to our samples to the file that was just created. We can achieve this with </p> <pre><code>ls fastq | grep -oE 'BB_[0-9]+' | sort -t _ -k 2 -n | uniq | \\\n      grep -vE 'BB_[38]' | \\\n      awk -v path=$(pwd)/fastq/ 'OFS=\"\\t\" \\\n          {print $1, path $1 \"_1.fastq.gz\", path $1 \"_2.fastq.gz\"}' \\\n      &gt;&gt; fastq_abs_paths\n</code></pre> <p>Info</p> <p>With the first grep we get the sample IDs from the filenames. We then sort them numerically (-n) based on the second field (-k 2) when split at underscores (-t _). Since there are two files per sample, we only keep the uniq sample IDs before removing the low-read-counts samples (BB_3 and BB_8) with another grep (grep -v keeps all lines that do not match the regex). The remaining sample IDs are subsequently fed to awk in order to print the absolute paths which are finally appended to fastq_abs_paths. </p> <p>This should have done the trick. <code>cat fastq_abs_paths</code> let's us see what we got. The output should look like this: </p> <pre><code>sample-id       forward-absolute-filepath       reverse-absolute-filepath\nBB_1    /home/user/data/microbiome/fastq/BB_1_1.fastq.gz      /home/user/data/microbiome/fastq/BB_1_2.fastq.gz\nBB_2    /home/user/data/microbiome/fastq/BB_2_1.fastq.gz      /home/user/data/microbiome/fastq/BB_2_2.fastq.gz\nBB_4    /home/user/data/microbiome/fastq/BB_4_1.fastq.gz      /home/user/data/microbiome/fastq/BB_4_2.fastq.gz\nBB_5    /home/user/data/microbiome/fastq/BB_5_1.fastq.gz      /home/user/data/microbiome/fastq/BB_5_2.fastq.gz\nBB_6    /home/user/data/microbiome/fastq/BB_6_1.fastq.gz      /home/user/data/microbiome/fastq/BB_6_2.fastq.gz\nBB_7    /home/user/data/microbiome/fastq/BB_7_1.fastq.gz      /home/user/data/microbiome/fastq/BB_7_2.fastq.gz\nBB_9    /home/user/data/microbiome/fastq/BB_9_1.fastq.gz      /home/user/data/microbiome/fastq/BB_9_2.fastq.gz\nBB_10   /home/user/data/microbiome/fastq/BB_10_1.fastq.gz     /home/user/data/microbiome/fastq/BB_10_2.fastq.gz\nBB_11   /home/user/data/microbiome/fastq/BB_11_1.fastq.gz     /home/user/data/microbiome/fastq/BB_11_2.fastq.gz\nBB_12   /home/user/data/microbiome/fastq/BB_12_1.fastq.gz     /home/user/data/microbiome/fastq/BB_12_2.fastq.gz\n</code></pre> <p>Great! This should be sufficient to let QIIME2 know where the files that we want to import are. Now, we can import the reads with </p> <pre><code>qiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path fastq_abs_paths \\\n    --output-path fastq_imported.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\n</code></pre> <p>This hopefully finishes successfully in a few seconds. Afterwards, you can check whether a new file was created with <code>ls</code> (which is slowly becoming our best friend now \u2013 right after <code>cd</code> of course). </p> <p>qiime also requires the metadata to be in TSV (tab-separated values), whereas our file is a CSV (comma-separated). We can simply fix this with </p> <pre><code>cat meta.csv | tr ',' '\\t' &gt; meta.tsv\n</code></pre>"},{"location":"microbiome/#denoising-with-dada2","title":"Denoising with DADA2","text":"<p>Now that we have imported the data we can unleash the power of qiime! Sequence denoising (or OTU clustering) is the centrepiece of every 16S pipeline. We will use the DADA2 which fits a sequencing error model to the data and tries to merge (\"denoise\") sequences that differ only due to sequencing errors as opposed to actual biological variation. </p> <pre><code>qiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs fastq_imported.qza \\\n    --p-trunc-len-f 190 \\\n    --p-trunc-len-r 190 \\\n    --p-n-threads 1 \\\n    --verbose \\\n    --o-table table.qza \\\n    --o-representative-sequences rep_seqs.qza \\\n    --o-denoising-stats denoising_stats.qza\n</code></pre> <p>This will produce an artefact holding a list of unique sequences (rep_seqs.qza) as well as a table with the number of occurrences of each representative sequence per sample (table.qza). DADA2 fits the error model on all reads of a sequencing run simultaneously (as opposed to Deblur, which fits it separately for each sample). It is therefore considerably slower and we will have to wait a few minutes for it to finish. In the meantime you can have a look at the later sections of the practical. </p> <p>Info</p> <p>If applicable, increase the number of threads in order to speed things up. Hint: You can put time in front of any command to see how long it took. Try it with <code>time sleep 5</code> in a new terminal. Also note that we told the program to truncate forward and reverse reads after 190 bp due to the decrease in quality we saw in multiqc_report.html. As the amplicon is only expected to be ~310 bp long, this should still give us sufficient overlap. </p>"},{"location":"microbiome/#building-a-tree","title":"Building a tree","text":"<p>qiime includes a tree-building pipeline which allows us to generate a phylogenetic tree from the denoised sequences with a single command. MAFFT is used for the alignment and multiple tree-inference methods are available (have a look at qiime's phylogeny plugin for details). We will use FastTree, which is the fastest but also least accurate option available. </p> <pre><code>qiime phylogeny align-to-tree-mafft-fasttree \\\n    --i-sequences rep_seqs.qza \\\n    --o-alignment aligned_rep_seqs.qza \\\n    --o-masked-alignment masked_aligned_rep_seqs.qza \\\n    --o-tree unrooted_tree.qza \\\n    --o-rooted-tree rooted_tree.qza\n</code></pre> <p>As you can see, this will create an alignment, mask locations that aligned badly, and then generate the tree (unrooted and rooted at midpoint). </p> <p>Info</p> <p>We have built a new tree here because the pipeline with FastTree runs quite quickly and we don't want to have to wait during the practical. However, instead of creating one from scratch, we could have also inserted our sequences into an existing phylogeny. qiime offers pre-computed trees for two popular 16S databases and an insertion algorithm in the fragment-insertion plugin. In general, it needs to be said that phylogenomics is an incredibly deep topic. Since the tree is not substantial for our analysis, we can simply use qiime's pipeline with the default parameters. However, if you ever rely on a high-quality phylogeny for a different project, you should definitely try to find the best approach for your data and have a close look at the respective literature. </p>"},{"location":"microbiome/#estimating-diversity","title":"Estimating diversity","text":"<p>One reason for generating a phylogeny in a microbiome analysis is so that it can be used in phylogeny-based diversity metrics. Let's generate these now. Before we can run the corresponding command, though, we need to look up the lowest number of denoised reads per sample. </p> <p>Info</p> <p>Large differences in sequencing depth between samples can distort the results of diversity estimates. Therefore, it is common practice to down-sample (in ecology-speech \"rarify\") the reads of each sample to a number that is equal or smaller than the number of reads in the least deeply sequenced sample. This simply means that we randomly select N reads from each sample where N is the smallest number of reads in any sample. </p> <p>To check the number of denoised reads per sample, we can create a qiime visualisation of our counts table with </p> <pre><code>qiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\n</code></pre> <p>Visualisation files are produced by certain qiime commands and provide human-readable information like plots, tables, or summary statistics. There are several ways to view such files. The easiest one is to go to https://view.qiime2.org/ and drag &amp; drop them into your browser window. </p> <p>Note that the tables and plots generated at https://view.qiime2.org/ are all rendered in your local browser and that nothing is uploaded to be processed on an external server, which is often required when working with sensitive data. </p> <p>Try using this site to view the table.qzv visualisation artefact produced by the last command. Once the visualisation has loaded, there should be a table looking like this: </p> <p></p> <p>So, we need to rarify to a sampling depth of 24,336 reads. Let's generate the diversity metrics now (again, adjust the number of threads according to your setup): </p> <pre><code>qiime diversity core-metrics-phylogenetic \\\n    --i-phylogeny rooted_tree.qza \\\n    --i-table table.qza \\\n    --p-sampling-depth 24366 \\\n    --p-n-jobs-or-threads 1 \\\n    --m-metadata-file meta.tsv \\\n    --output-dir core-metrics-results\n</code></pre> <p>This will generate a new directory \"core-metrics-results\" holding (based on multiple different diversity metrics) sample-wise diversity values (\"alpha diversity\"), pairwise inter-sample distance matrices (\"beta diversity\"), and visualisations of PCoA plots (ending in .qzv). These can again be inspected with https://view.qiime2.org/. For example, the PCoA plot based on Bray\u2013Curtis distance with BV-negative samples in red and BV-positive samples in blue looks like this: </p> <p></p> <p>Question</p> <p>What does the plot tell us about our samples and the impact of BV on inter-sample diversity. Look up the term \"Anna Karenina Principle\" and what it means in terms of the microbiome. Can we say that it applies to our data? </p>"},{"location":"microbiome/#taxonomic-classification","title":"Taxonomic classification","text":"<p>We are not only interested in the ecological diversity of our samples; we also want to know which species were found. Again, there are multiple ways of achieving this. We will use a Na\u00efve Bayes classifier (pre-trained on the Greengenes 13_8 database) available from the Qiime2 website. You can find it in the db directory and use it with the following command: </p> <pre><code>qiime feature-classifier classify-sklearn \\\n    --i-classifier db/gg-13-8-99-nb-classifier.qza \\\n    --i-reads rep_seqs.qza \\\n    --p-n-jobs 1 \\\n    --o-classification taxonomy.qza\n</code></pre> <p>The produced table in taxonomy.qza simply links the taxonomic classifications (from phylum to species level) to the corresponding sequences. If you want to inspect the table, run </p> <pre><code>qiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n</code></pre> <p>and open the visualisation taxonomy.qzv in https://view.qiime2.org/. </p> <p>To get a more intuitive understanding of the microbial composition of our samples we can now ask qiime to plot it for us: </p> <pre><code>qiime taxa barplot \\\n    --i-table table.qza \\\n    --i-taxonomy taxonomy.qza \\\n    --m-metadata-file meta.tsv \\\n    --o-visualization taxa_barplot.qzv\n</code></pre> <p>The resulting visualisation at species level (\"Level 7\") looks like this in https://view.qiime2.org/: </p> <p></p> <p>Info</p> <p>You can adjust the with of the bars with the slider above the plot. </p> <p>Question</p> <p>We can see that some samples are dominated by Lactobacillus iners (green), whereas for others the situation looks very different. Double check with the metadata to find out if this is associated with BV-status. </p>"},{"location":"microbiome/#testing-differences-in-alpha-diversity","title":"Testing differences in alpha diversity","text":"<p>Alpha diversity measures the general diversity of an ecosystem (i.e. a sample in our case) or a group of ecosystems (i.e. groups of samples like all samples with BV). After looking at the taxa barplot we just generated, do you think that the BV samples are statistically significantly more diverse than the non-BV samples? We can check if your estimate is correct by running a Kruskal\u2013Wallis test on the four metrics of alpha diversity that we calculated for our samples. Let's create a new directory to write the results into and run: </p> <pre><code>mkdir alpha_tests\nfor metric in faith_pd evenness shannon observed_features; do\n    qiime diversity alpha-group-significance \\\n        --i-alpha-diversity core-metrics-results/${metric}_vector.qza \\\n        --m-metadata-file meta.tsv \\\n        --o-visualization alpha_tests/${metric}_group_significance.qzv\ndone\n</code></pre> <p>This produces four more visualisations. Have a look at them. Were you right? </p> <p>Question</p> <p>It looks like the different alpha diversity metrics disagree! After reading the short definitions of the metrics provided below, can you think of a reason for this discrepancy? faith_pd: Faith's phylogenetic diversity is defined as the sum of branch-lengths in the phylogeny between all species found in a sample (regardless of abundance). evenness: Pielou's evenness index quantifies how differently abundant species making up an ecosystem are. shannon: The Shannon index or Shannon entropy quantifies how difficult it is to guess the species of a random specimen taken from the sample (the more species and the more equally abundant they are, the more difficult). observed_features: Simply the number of unique taxa found in the sample. </p>"},{"location":"microbiome/#testing-beta-diversity","title":"Testing beta diversity","text":"<p>As opposed to alpha diversity, which quantifies the diversity of a sample (or a group of samples) overall, beta diversity gives an estimate of the magnitude of differences between individual samples or groups of samples. We can test the difference between BV and non-BV samples for all beta diversity metrics with the following command: </p> <pre><code>mkdir -p beta_tests\n\nfor metric in bray_curtis jaccard unweighted_unifrac weighted_unifrac; do\n    qiime diversity beta-group-significance \\\n        --i-distance-matrix core-metrics-results/${metric}_distance_matrix.qza \\\n        --m-metadata-file meta.tsv \\\n        --m-metadata-column BV \\\n        --o-visualization beta_tests/${metric}_significance.qzv \\\n        --p-permutations 9999\ndone\n</code></pre> <p>Again, we got a visualisation for each metric. Have a look at them in https://view.qiime2.org/. What do you find? Are all of them in agreement this time? </p> <p>This concludes today's practical. If you are interested in differentially abundant taxa between the BV and non-BV samples, have a look at qiime's' ANCOM function. </p> <p>Acknowledgements: Many thanks to Dr. Suzanna Francis for providing the data and Ernest Diez-Benavente and Julian Lisebber-Egger for designing the practical materials.</p>"},{"location":"ont-assembly/","title":"ONT Assembly","text":""},{"location":"ont-assembly/#overview","title":"Overview","text":"<p>In this session we will be performing a de novo assembly of a genome using ONT reads. We will be using the <code>flye</code> assembler, which is specifically designed for long-read sequencing data. You will contrast these methods and the output of the assembly with the output of a short-read assembly using <code>spades</code> that you generated earlier in the course.</p>"},{"location":"ont-assembly/#software-and-data-requirements","title":"Software and Data Requirements","text":"<p>We will be using the following software and data in this activity:</p> <ol> <li>Nanoplot - to perform read quality control</li> <li>Flye - to perform de novo assembly of the genome</li> <li>Quast - to perform assembly quality control</li> <li>Prokka - to annotate the assembly</li> </ol> <p>We will now use mamba to create an environment to install the required software. </p> <pre><code>mamba create -n ont-assembly flye quast prokka nanoplot\n</code></pre> <p>After it has finished you can activate the environment using:</p> <pre><code>mamba activate ont-assembly\n</code></pre> <p>We will also be using data from the this study. </p> <p>We are going to be assembling the data for one of the samples from this study. You should download the fastq data from the ENA browser. The data is available under the accession <code>ERR12245902</code>. </p> <p>Create a folder called <code>ont-assembly</code> in your <code>data</code> directory. After you download the data, you can move it from your <code>Downloads</code> folder to the <code>ont-assembly</code> folder. </p>"},{"location":"ont-assembly/#read-qc","title":"Read QC","text":"<p>Try to use nanoplot to summarise the quality of the reads. </p> <p>Question</p> <p>What is the average read length of the reads?</p>"},{"location":"ont-assembly/#assembly","title":"Assembly","text":"<p>We will be using the <code>flye</code> assembler to perform the assembly. The a template of the command is as follows:</p> <pre><code>flye --nano-hq &lt;fastq&gt; --out-dir &lt;output_dir&gt; --threads 2 \n</code></pre> <p>Where <code>&lt;fastq&gt;</code> is the path to the fastq file, <code>&lt;output_dir&gt;</code> is the path to the output directory and <code>--threads</code> is the number of threads to use. Flye will create a file called <code>assembly.fasta</code> in the output directory. This file contains the assembled genome, which is a collection of contigs.</p>"},{"location":"ont-assembly/#assembly-qc","title":"Assembly QC","text":"<p>We will be using <code>quast</code> to perform the assembly quality control. The command is as follows:</p> <pre><code>quast.py &lt;assembly.fasta&gt; -o &lt;output_dir&gt; --threads 2\n</code></pre> <p>Where <code>&lt;assembly.fasta&gt;</code> is the path to the assembly file and <code>&lt;output_dir&gt;</code> is the path to the output directory.</p> <p>Quast will generate a number of files in the output directory. Have a look at the <code>report.html</code> file by opening it via the file browser or using the command:</p> <pre><code>firefox &lt;output_dir&gt;/report.html\n</code></pre> <p>Question</p> <p>What is the N50 of the assembly?</p> <p>What is the number of contigs in the assembly?</p> <p>What is the total length of the assembly? How does this compare with the expected Mtb genome size?</p> <p>How to these metrics compare to the short-read assembly you generated earlier in the course?</p>"},{"location":"ont-assembly/#annotation","title":"Annotation","text":"<p>We will be using <code>prokka</code> to annotate the assembly. The command is as follows:</p> <pre><code>prokka --outdir &lt;output_dir&gt;  &lt;assembly.fasta&gt; --cpus 2\n</code></pre> <p>Where <code>&lt;output_dir&gt;</code> is the path to the output directory, <code>&lt;assembly.fasta&gt;</code> is the path to the assembly file and <code>--cpus</code> is the number of threads to use. In the output directory there will be several files, including files that end in:</p> <ol> <li>.fna - a fasta file with the nucleotide sequence of the genome</li> <li>.ffn - a fasta file with the nucleotide sequence of the coding regions</li> <li>.faa - a fasta file with the amino acid sequence of the coding regions</li> <li>.gff - a text file with the location of genes </li> </ol>"},{"location":"ont-assembly/#comparing-protein-sequences","title":"Comparing protein sequences","text":"<p>Open up the <code>.faa</code> file in a text editor (by double clicking on it in the file browser or running <code>open &lt;filename&gt;</code> on the terminal). You will see that the file contains the protein sequences of the coding regions. </p> <p>As we saw in the previous session, RpoB is a protein that is the target of the rifampicin antibiotic. We can use the protein sequence of RpoB to identify the presence of mutations that confer resistance to rifampicin. Look for the protein sequence of RpoB in the <code>.faa</code> file. You can search for it by using the command <code>ctrl + f</code> and typing <code>RNA polymerase subunit beta</code>. </p> <p>On the terminal run <code>aliview</code> to open the aliview program. Copy in the protein sequence of RpoB from the <code>.faa</code> file. You can do this by selecting the sequence in the text editor and using <code>ctrl + c</code> to copy it. Then in aliview, use <code>ctrl + v</code> to paste it.</p> <p>Now copy the protein sequence from https://mycobrowser.epfl.ch/genes/Rv0667 and paste it into aliview.</p> <p>Question</p> <p>The two sequences are exactly same length, so in this case we don't need to align them. What are the differences between the two sequences? Are any of the differences in the region that is known to confer resistance to rifampicin? (use the table from the TB genomics session to help you with this).</p>"},{"location":"ont/","title":"Third Generation Sequencing","text":"<p>Important</p> <p>Before we start we need to do a little update of your virtual machine. Please run the following command in the terminal: <pre><code>bash &lt;(curl -Ssk https://raw.githubusercontent.com/lshtm-genomics/pathogen-genomics/main/scripts/update-vm.sh)  \n</code></pre></p>"},{"location":"ont/#introduction","title":"Introduction","text":"<p>In this session we are going to be looking at data generated by third-generation nanopore sequencing technology. Developed by Oxford Nanopore Technologies (ONT), these platforms, rather than the next-generation 'sequencing-by-synthesis approach', make use of an array of microscopic protein \u2018pores\u2019 set in in an electrically resistant membrane which guide strands of DNA or RNA through them. Each nanopore corresponds to its own electrode connected to a channel and sensor chip, which measures the electric current that flows through the nanopore. When a molecule passes through a nanopore, the current is disrupted to produce a characteristic \u2018squiggle\u2019. The squiggle is then decoded using basecalling algorithms to determine the DNA or RNA sequence in real time. Oxford Nanopore\u2019s most popular platform is the MinION which is capable of generating single reads of up to 2.3 Mb (2.3 million bases).</p> <p></p> <p>The MinION is one of 5 scalable platforms developed by ONT. High-throughput applications such as the GridION and PromethION use an array of nanopore flowcells to produce between 5 to 48 times more data than the MinION alone \u2013 outputting up to 48 TB of data in one run. More downscaled solutions such as The Flongle and SmidgION use a smaller, single flowcell to generate data. The MinION is a highly portable sequencing platform, about the size of a large USB flash drive. This technology enables researchers to perform sequencing analyses almost anywhere, providing they have the correct equipment to prepare the DNA libraries and analyse the output data.</p> <p></p> <p>A complete sequencing run on the MinION platform can generate upwards to of 1TB of raw data, and downstream analyses require a significant amount of computing power, multicore high performance processors and large amounts of RAM. This poses a significant logistical challenge for researchers who want to take advantage of the platform\u2019s portability aspect. Over recent years, the integration of GPUs (graphics processing units) has made it easier to analysis workflows.</p>"},{"location":"ont/#software-and-data-requirements","title":"Software and Data Requirements","text":"<p>We will be using the following software and data in this activity:</p> <ol> <li>NanoPlot - to visualise the summary statistics of the sequencing run</li> <li>chopper - for quality control</li> <li>kraken2 - to identify the taxonomic classification of the reads</li> <li>minimap2 - to map the reads to a reference genome</li> <li>samtools - to manipulate the alignment files</li> <li>freebayes - to call variants from the alignment files</li> <li>bcftools - to manipulate the variant calling files</li> <li>mafft - to align the genome sequences</li> <li>iqtree - to infer the phylogenetic tree</li> <li>figtree - to visualise the phylogenetic tree</li> <li>igv - to visualise the alignment files</li> <li>aliview - to visualise the alignment files</li> <li>R - to generate plots</li> </ol> <p>We will now use mamba to create an environment to install the required software. </p> <pre><code>mamba create -n ont nanoplot chopper kraken2 minimap2 samtools bcftools freebayes mafft iqtree igv figtree aliview r-base\n</code></pre> <p>We will also be using data from the this study. </p> <p>Question</p> <p>Have a look at the paper abstract and the methods section to get a feel for the data we will be working with. The methods state that amplicon sequencing was used. Why do you think this is? Which sequencing kit was used to generate the data?</p> <p>We will use data from two samples. The data is available from the ENA under the following accessions:</p> <ol> <li>SRR26779702 - We will first use this sample to demonstrate the analysis pipeline.</li> <li>SRR26779678 - You will use this sample to perform the analysis yourself.</li> </ol> <p>Download the fastq data and put it into the folder <code>~/data/nanopore_activity/</code>. </p>"},{"location":"ont/#activity-briefing","title":"Activity Briefing","text":"<p>We will follow a typical nanopore data analysis pipeline to produce a whole-genome Zika virus sequence to explore the capabilities of nanopore sequencing and the kind of data it can produce. We will then use maximum-likelihood (ML) techniques for phylogeographic reconstruction, to try and find out where our isolates came from and place it in the context of the wider South American Zika.</p>"},{"location":"ont/#basecalling","title":"Basecalling","text":"<p>Basecalling is performed with a tool called <code>dorado</code>. This tool is used to convert the raw signal data generated by the MinION into a sequence of nucleotides. The basecaller uses a neural network to predict the sequence of bases from the raw signal data. The output of the basecaller is a fastq file, which contains the basecalled sequence, as well as other information about the read, such as the quality scores of the basecalls. </p> <p>Basecalling required the use of advanced machine learning models, and can be computationally intensive. For this reason, basecalling is often performed on a high-performance computing cluster with a graphics processing unit. In this activity, we will use a pre-basecalled dataset to save time. If you want to learn more about dorado, check out their github page.</p> <p> </p>"},{"location":"ont/#quality-control","title":"Quality Control","text":"<p>Before moving on to the analysis steps, it is important to gauge the quality of your sequencing output. There are numerous factors which dictate the quality of the output data, spanning between quality of the input material, library preparation to software and hardware failure. We will look at some important metrics that can be calculated from the sequencing data to assess the quality of the run.</p> <p>In order to get the run metrics in to a useful form, we will use <code>NanoPlot</code> to produce a range of plots in a HTML output, which we will use to judge the quality of the sequencing run. </p> <pre><code>NanoPlot --fastq SRR26779702_1.fastq.gz -o SRR26779702_qc --N50 \n</code></pre> <p>Info</p> <p>After executing the command you should find a file called 'SRR26779702_qc/NanoPlot-report.html'. Open them up in the file manager or in the terminal (with the below command) and inspect some of the plots and see what you can find out. </p> <pre><code>firefox SRR26779702_qc/NanoPlot-report.html\n</code></pre> <p>Before continuing, quit firefox by clicking the X in the top right corner of the web-browser window.</p> <p>Question</p> <p>What is the N50 of the reads? Do you think this is a good N50 value for this type of sequencing?</p>"},{"location":"ont/#quality-filtering","title":"Quality filtering","text":"<p>The next step in the analysis is to filter out any low-quality reads. This is important because low-quality reads can introduce errors in the downstream analyses, such as mapping and variant calling. We will use a tool called <code>chopper</code> to trim the reads and remove any low-quality bases. </p> <pre><code>chopper -q 10 -i SRR26779702_1.fastq.gz &gt; SRR26779702.filt.fastq\n</code></pre> <p>The <code>-q</code> flag specifies the quality threshold for trimming. In this case, we are using a threshold of 10, which means that any read with a quality score below 10 will be dropped. The <code>-i</code> flag specifies the input file, and the output file is specified with the <code>&gt;</code> symbol.</p> <p>Question</p> Question 3Answer 3 <p>How many reads were filtered out?</p> <p>12028 reads were filtered out. </p>"},{"location":"ont/#kraken-qc","title":"Kraken QC","text":"<p>Another method of quality control is to check our reads for sequence contamination from other 'off-target' organisms. This is important in order to firstly, understand how effective your DNA extraction, enrichment and sequencing was. And secondly, to prevent anomalous reads from being incorporated in to assemblies.</p> <p>Using our basecalled reads we will perform an analysis using kraken2. Kraken2 is a tool which sifts through each read in a .fastq file and crosschecks it against a database of microorganisms. The output is a taxonomic assignment of each read, enabling to identify if any contamination has occurred. In this case we will be looking for any reads which do not belong to the Zika genome. Type the following command in to the terminal to unleash the Kraken:</p> <pre><code>kraken2 --db db/ SRR26779702.filt.fastq --report SRR26779702.report.txt --output SRR26779702.output.txt\n</code></pre> <p>The <code>--db</code> flag specifies the database to use. In this case we are using a database with only viral sequences. The <code>--report</code> flag specifies the output file for the report, and the <code>--output</code> flag specifies the output file for the taxonomic assignments.</p>"},{"location":"ont/#kraken-report","title":"Kraken Report","text":"<p>The report generated by Kraken is a tab-delimited file which contains a list of all the reads in the input file, and the taxonomic assignment of each read. The first column is the read ID, the second is the taxonomic ID, the third is the length of the read, and the fourth is the lowest common ancestor (LCA) of the taxonomic assignment. The LCA is the lowest taxonomic rank that all the taxonomic assignments share. The report is useful for identifying the taxonomic assignments of reads, and for identifying any contamination in the dataset.</p>"},{"location":"ont/#kraken-output","title":"Kraken Output","text":"<p>The output generated by Kraken is a tab-delimited file which contains the taxonomic assignment of each read. The first column is the read ID, the second is the taxonomic ID, the third is the length of the read, and the fourth is the taxonomic assignment. The output is useful for identifying the taxonomic assignments of reads, and for identifying any contamination in the dataset.</p>"},{"location":"ont/#mapping-and-visualisation","title":"Mapping and Visualisation","text":"<p>Now that we have varified a successful sequencing run, our basecalled and QC'd Zika-confirmed data are ready to go, we will now map the reads on to a reference genome and perform variant calling. For the first step we will use the minimap2 program. You can find more information about this tool by clicking the link comparing the two alignment tools</p> <pre><code>minimap2 reference.fasta SRR26779702.filt.fastq -ax map-ont | samtools sort -o SRR26779702.bam -\n</code></pre> <p>This command contains two parts: </p> <ol> <li>the first part is the <code>minimap2</code> command, which is used to map the reads to the reference genome. The <code>-ax</code> flag specifies the type of alignment to perform. In this case, we are using the <code>map-ont</code> option, which is specifically designed for mapping ONT reads. </li> <li>The second part is the <code>samtools sort</code> command, which is used to sort the output of the minimap2 command. The <code>-o</code> flag specifies the output file, and the <code>samtools sort</code> command sorts the output by reference position.</li> </ol>"},{"location":"ont/#visualisation","title":"Visualisation","text":"<p>Now that we have successfully mapped the reads to a reference we can visualise them in IGV, to get a closer look at what our sequencing run looks like. Before we do that we need to index the bam file so that IGV can read it. </p> <pre><code>samtools index SRR26779702.bam\n</code></pre> <p>Next we can start IGV by typing the following command in to the terminal:</p> <pre><code>igv\n</code></pre> <p>As you have learned before, open up the reference (reference.fasta) in IGV and then load the bam file (SRR26779702.bam) in to the viewer.</p> <p>Question</p> <p>Can you see any difference between this data and the illumina data you have previously analysed? What do you notice about the coverage of the reads? Are there any regions which have a high depth of coverage? What do you notice about the quality of the reads? Can you see any regions which have a high frequency of mismatches?</p> <p>Info</p> <p>An important aspect of an effective nanopore sequencing analysis is being able to differentiate between errors or low quality basecalls, and true SNPs. You can see that there are random errors dotted around the screen, like static on a TV. Do you notice that some positions in the alignment have a distinct vertical column of red squares. These are most likely the variants we are looking for, and the key to unlocking our sequence data. These columns represent positions which have a high frequency of basecalls which do not agree with the reference sequence. It is unlikely that random errors will appear in such a manner, and so, in our next analysis we will use these high frequency variants to alter the reference sequence to build a new sequence, in a process called \u2018variant calling\u2019.</p>"},{"location":"ont/#depth-and-coverage","title":"Depth and Coverage","text":"<p>Info</p> <p>Variant calling is a process used to identify new genotypes based on the \u2018differences\u2019 found our read data. You will have used it in the previous mapping practical at the start of the course. In this case, we are going to be using the alignment you have just generated and compiling a database of SNPs, inferred from positions which have a majority allele which is different from the one found on the reference sequence.</p> <p>Before starting on variant calling, we first need to do one more QC step. This analysis will tell us how well our reads have aligned to the reference and how comprehensive our sequencing run was. Two key metrics are required for this: reference coverage and read depth. Read coverage tells us the percentage of the reference which has had sequencing reads aligned to it, which allows us to identify any regions that may have not been successfully sequenced. Depth is an equally as important metric: it tells us how many different reads have mapped to the same position. This is a particularly important statistic if you intend on doing variant calling, as regions with low depth may fall prey to false calls due to the random errors we have in our nanopore data. With a high enough read depth, we can be fairly sure that these errors will be ignored.</p> <p>We will now use R to generate a plot to allow us to assess the coverage and depth.</p> <p>We need to use samtools, a versatile package you will be familiar with, to extract the depth statistics from the .bam alignment file we generated in the previous section. This will generate a file called SR26779702.depth.txt. The command is as follows:</p> <pre><code>samtools depth -a SRR26779702.bam &gt; SRR26779702.depth.txt\n</code></pre> <p>Next, we will use the R statistical package to generate a plot based on the data samtools generated. Simply type \u2018R\u2019 in to the terminal to initialise the R interface.</p> <pre><code>R\n</code></pre> <p></p> <p>Once you have initialised R, you can enter the following two lines of code, one after the other. The first command will load the \u2018 depth_statistics\u2019 file, and the second will generate the plot</p> <pre><code>data&lt;-read.table(\"SRR26779702.depth.txt\",header=F)\n</code></pre> <pre><code>plot(data$V3,type=\"l\",xlab=\"Reference Position\", ylab=\"read Depth\")\n</code></pre> <p>Info</p> <p>Take a look at the plot you have just generated in R. Does the genome have adequate coverage and depth? What do you notice about the depth of the reads of nanopore generated data, as opposed to Illumina? Furthermore, why do you think there are these block-like increases and decreases in the depth? (Hint: how do you think these sequencing libraries were generated?)</p> <p>You can also calculate some basic statistics about the depth of coverage. The following command will calculate the min, Q1, median, Q3 and max of the depth of coverage:</p> <pre><code>summary(data$V3)\n</code></pre> <p>Question</p> Question 4Answer 4 <p>What is the minimum depth of coverage?</p> <p>73</p> <p>You can quit R by typing:</p> <pre><code>quit()\n</code></pre>"},{"location":"ont/#variant-calling","title":"Variant Calling","text":"<p>Info</p> <p>Now that we know the coverage and depth, we can move on to the variant calling. For this, we will use a package called freebayes. Freebayes will look through the alignment and \u2018call\u2019 the positions which do not agree with the reference, count them and compile them in to a database called a VCF. Enter the following code in to the terminal to begin the process:</p> <pre><code>freebayes -f reference.fasta SRR26779702.bam  -F 0.6 --ploidy 1 | bcftools view -Oz -o SRR26779702.vcf.gz\n</code></pre> <p>The <code>-f</code> flag specifies the reference genome to use, and the <code>-F</code> flag specifies the minimum allele frequency to call a variant. In this case, we are using a threshold of 0.6, which means that any position with an allele frequency below 0.6 will be ignored. The <code>--ploidy</code> flag specifies the ploidy of the organism. In this case, we are using a ploidy of 1, which means that we are treating the organism as haploid. The <code>|</code> symbol is used to pipe the output of the freebayes command to the bcftools command. The <code>bcftools view</code> command is used to convert the output of freebayes to a VCF file. The <code>-Oz</code> flag specifies that the output should be compressed using gzip, and the <code>-o</code> flag specifies the output file name.</p> <p>Let's take a look inside the VCF file to see what kind of data it contains:</p> <pre><code>zcat SRR26779702.vcf.gz | less\n</code></pre> <p>Info</p> <p>You can scroll down using the down-arrow key on your keyboard. Can you recall the common features of a VCF file? Press 'q' to quit the 'zless' program to continue with the activity:</p> <p>Next, we need to index the .vcf.gz file. This is a necessary step before we can use the VCF file in the next step of the analysis. The command is as follows:</p> <pre><code>bcftools index SRR26779702.vcf.gz\n</code></pre>"},{"location":"ont/#consensus-generation","title":"Consensus generation","text":"<p>Important</p> <p>It is important to understand how the reference sequence can effect the consensus outcome. If unchecked, positions with zero coverage will default to reference. This will result in the experimental sequence becoming more like the reference than it actually is. We can use BEDtools mark positions with low coverage which BCFtools will use to mask the zero coverage positions with 'N's. </p> <p>We will first generate a bed file with the positions of the low coverage regions. The command is as follows:</p> <pre><code>awk '$3&lt;100' SRR26779702.depth.txt | awk '{print $1\"\\t\"$2-1\"\\t\"$2}' &gt; SRR26779702.low-dp.bed\n</code></pre> <p>This command will generate a bed file with the positions of the low coverage regions. The first awk command will filter the depth file to only include positions with a depth of less than 100. The second awk command will format the output to be in bed format, which is required by bedtools. Bed format is a tab-delimited format which contains the chromosome name, start position and end position of regions of interest, which are the low coverage regions in this case.</p> <p>This next line of code will take the calls made in the VCF file and apply them to the reference sequence, changing it based of the differences observe in our reads:</p> <pre><code>bcftools consensus -f reference.fasta -p \"SRR26779702 \" --mask SRR26779702.low-dp.bed -M N SRR26779702.vcf.gz  &gt; SRR26779702.consensus.fasta\n</code></pre> <p>This command will generate a new fasta file called \u2018SRR26779702.consensus.fasta\u2019. This file contains the modified reference sequence, which has been altered based on the differences observed in our reads. The <code>-p</code> flag specifies the prefix for the sequence name in output file, and the <code>-f</code> flag specifies the reference genome to use. The <code>--mask</code> flag specifies the bed file to use, and the <code>-M</code> flag specifies the character to use for low coverage positions. In this case, we are using 'N' to mask the low coverage positions.</p> <p>Info</p> <p>You will now have a file called \u2018consensus_sequence.fasta\u2019. this contains the modified reference, a brand new sequences based on the sequence data we have been working with.</p>"},{"location":"ont/#now-do-it-all-again","title":"Now do it all again!","text":"<p>Now that you have completed the first part of the activity, you will now repeat the process with a different sample. The sample you will be using is SRR26779678. You will need to download the fastq file from the ENA and place it in the same folder as before.</p>"},{"location":"ont/#sequence-alignment","title":"Sequence Alignment","text":"<p>Now we are going add our consensus sequence to a multiple sequence alignment, to prepare the dataset for phylogenetic inference. The <code>zika_dataset.fasta</code> file contains several publicly available ZIKV isolates. Let\u2019s open up the dataset in an alignment viewer and take a look at that this kind of data looks like. Like most of the tools you have encountered during this course there are many alternative programs out there that will perform similar tasks, in this instance we will use Aliview. Let\u2019s call up the program and load our dataset:</p> <pre><code>aliview zika_dataset.fasta\n</code></pre> <p></p> <p>With Aliview open, you should now see the dataset we will be working with for the rest of the activity. In the left pane you should see the sequence metadata: the unique sequence accession number, the species the virus was isolated from, the country of origin and the date of collection. Spanning across horizontally, you should see the genome sequence to which the metadata is associated with. On the pane at the bottom of the window, you should see a few statistics about the dataset. Have a browse and familiarise yourself with the data and the software interface.</p> <p>Question</p> Question 5Answer 5 <p>How many sequences are there in the Zika dataset?</p> <p>284</p> <p>Before we continue with any analyses, we first have to add our new sequences and then align the dataset. Alignment is the process of arranging sequence data in such a way that each sequence may be compared to each other. For this we will use a program called \u2018mafft\u2019. This program will rearrange the sequences based on similarity, so that they may be compared in future analyses.</p> <p>Close the Aliview window, and open up the terminal ensuring you are still in the \u2018phylogenetics\u2019 directory. First, we will add our new consensus sequence to the database by using the \u2018cat\u2019 (concatenate) command:</p> <pre><code>cat SRR26779702.consensus.fasta SRR26779678.consensus.fasta zika_dataset.fasta &gt; unaligned.fasta\n</code></pre> <p>Next, we will call up the alignment program and point it in the direction of our unaligned \u2018unaligned.fasta\u2019 dataset. The alignment command is made up of the following elements:</p> <pre><code>mafft unaligned.fasta &gt; zika_all_aligned.fasta\n</code></pre> <p>The <code>mafft</code> command calls the alignment program, and the <code>&gt;</code> symbol specifies the output file. The <code>zika_all.fasta</code> file is the input file, and the <code>zika_all_aligned.fasta</code> file is the output file.</p> <p>The program may take a short while to run. Alignments generally can take a very long time; some of you may be familiar with using the ClustalO or MUSCLE web-based servers. While these are a few of the more popular options, MAFFT has its merits, particularly in being able to build large alignments in an exceptionally short amount of time.</p> <p>Info</p> <p>Once the program has completed, you should find a file called \u2018zika_all_ailigned.fasta\u2019. Open it up in Aliview and inspect the sequences. Comparing this to the unaligned dataset, can you see how easy it is now, to compare each sequence and visualise the diversity across the dataset. With our sequence added to the dataset, we can now move on to the final section of the activity: phylogenetics.</p>"},{"location":"ont/#phylogenetics-iqtree","title":"Phylogenetics: iqtree","text":"<p>IQ-Tree is the tool we will be using today to infer our phylogenetic tree. It is a popular program for phylogenetic analyses of large datasets. Its main strengths are its speed and a robust search algorithm resulting in phylogenies with good likelihood scores. The maximum likelihood method uses standard statistical techniques for inferring probability distributions, and to assign probabilities to particular possible phylogenetic tree topologies (the shape of the tree). Hopefully you should be familiar with the basic concepts underpinning phylogenetics, and their applications: so let\u2019s get stuck in.</p> <p>Now we have our multiple sequence alignment, we can now start the phylogenetic inference. The following command consists of these elements:</p> <pre><code>iqtree -s ./zika_all_aligned.fasta -bb 1000 -nt AUTO\n</code></pre> <p>The <code>-s</code> flag specifies the input file, and the <code>-bb</code> flag specifies the number of bootstrap replicates to perform. In this case, we are using 1000 bootstrap replicates, which is a common number for phylogenetic analyses. The <code>-nt</code> flag specifies the number of threads to use for the analysis. In this case, we are using AUTO, which will automatically detect the number of threads available on your machine. Choosing the right model of evolution is an important step in phylogenetic inference. IQ-Tree will automatically select the best model of evolution for your dataset, based on the Akaike Information Criterion (AIC). The AIC is a statistical method for selecting the best model of evolution based on the likelihood of the data given the model. The AIC is a measure of how well the model fits the data, and it penalises models with more parameters. The model with the lowest AIC score is selected as the best model. If you already know which model you want to use, you can specify it with the <code>-m</code> flag. For example, if you want to use the GTR+F+G4 model, you can use the following command:</p> <pre><code>iqtree -s ./zika_all_aligned.fasta -m GTR+F+G4 -bb 1000 -nt AUTO\n</code></pre> <p>Info</p> <p>If you have any questions about this phylogenetic application, or any others you are aware of, don't hesitate to ask a demonstrator.</p> <p>Now that iqtree has completed its run we can now visualise the results in Figtree \u2013 a phylogenetic tree visualisation software package. We will use the following command to open up the tree in Figtree:</p> <pre><code>figtree ./zika_all_aligned.fasta.treefile\n</code></pre> <p></p> <p>Now that you have opened up the tree in Figtree, you first need to assign the bootstrap data to a category. We can then organise the tree in a way that it is more easy to view. Using the animation below as a guide, select the 'Midpoint Root' and 'Increasing Node Order' settings.</p> <p></p> <p>Use the scroll bars in the Figree window, and on the pane on the left hand side of the windows to zoom and navigate the tree.</p> <p>You can use the search bar in the top right corner of the screen to find our 'CONSENSUS' sequence.</p> <p>Info</p> <p>Now that we can see where our sequence falls on the tree, we can look in to the geographical and chronological metadata in our dataset and the structure of the tree, to gain a deeper understanding of what happened during the Zika outbreak in the Americas.</p> <p>Question</p> Question 6Answer 6 <p>What is the closest ancestor of our sequence?</p> <p>Brazil - 13/04/2016</p> <p>Info</p> <p>What can you tell about the outbreak from the topology (shape and order) of the tree? Can you see distinct clades (groupings) in the overall structure of the tree? As you follow the nodes in each clade further in to the tree, towards the leaves, what to notice about the geography of the isolates in this dataset? Can you tell where most of the lineages began?</p> <p>We can use the phylogenies similar to the one we have just generated to map transmission and the spread of diseases.</p> <p>Info</p> <p>Click on this link to a reconstruction of the outbreak. This was built using a similar Zika dataset that we have used today, with a few extra added isolates. To generate this map, we used Bayesian ancestral state reconstruction with a molecular clock to map the spread of Zika during the 2015-16 outbreak. Do you notice the similarities between the structure of the tree in Figtree and the phylogeographic map?</p>"},{"location":"ont/#conclusion","title":"Conclusion","text":"<p>Hopefully having completed this activity, you will have gained a better understanding of how to process third-generation nanopore data, and the applications of such a technologies in an infectious disease outbreak scenario. The pipeline we have used in this activity is one of a hundred you can use to carry out similar tasks, but choosing the right tools for the job is paramount when performing sequence analysis.</p> <p>The use of front-line genomics during outbreaks is a somewhat novel practice, but with tools such as the MinION and its downstream applications, it is possible to monitor the dynamics of disease outbreaks in real-time to inform on-the-ground containment strategy and provide a framework for surveillance, both in a preventative and a post-outbreak context.</p> <p>If you have any further questions about this activity or your own applications of this skills learnt during this course, ask a demonstrator.</p>"},{"location":"ont/#presentation","title":"Presentation","text":"<p>Here is a link to the presentation slides used during the activity: MinION2025.pdf</p>"},{"location":"ont/#cheat-codes","title":"Cheat codes","text":"<pre><code>NanoPlot --fastq SRR26779702_1.fastq.gz -o SRR26779702_qc --N50 \nchopper -q 10 -i SRR26779702_1.fastq.gz &gt; SRR26779702.filt.fastq\nkraken2 --db db/ SRR26779702.filt.fastq --report SRR26779702.report.txt --output SRR26779702.output.txt\nminimap2 reference.fasta SRR26779702.filt.fastq -ax map-ont | samtools sort -o SRR26779702.bam -\nsamtools index SRR26779702.bam\nsamtools depth -a SRR26779702.bam &gt; SRR26779702.depth.txt\nfreebayes -f reference.fasta SRR26779702.bam  -F 0.6 --ploidy 1 | bcftools view -Oz -o SRR26779702.vcf.gz\nbcftools index SRR26779702.vcf.gz\nawk '$3&lt;100' SRR26779702.depth.txt | awk '{print $1\"\\t\"$2-1\"\\t\"$2}' &gt; SRR26779702.low-dp.bed\nbcftools consensus -f reference.fasta -p \"SRR26779702 \" --mask SRR26779702.low-dp.bed -M N SRR26779702.vcf.gz  &gt; SRR26779702.consensus.fasta\n\nNanoPlot --fastq SRR26779678_1.fastq.gz -o SRR26779678_qc --N50 \nchopper -q 10 -i SRR26779678_1.fastq.gz &gt; SRR26779678.filt.fastq\nkraken2 --db db/ SRR26779678.filt.fastq --report SRR26779678.report.txt --output SRR26779678.output.txt\nminimap2 reference.fasta SRR26779678.filt.fastq -ax map-ont | samtools sort -o SRR26779678.bam -\nsamtools index SRR26779678.bam\nsamtools depth -a SRR26779678.bam &gt; SRR26779678.depth.txt\nfreebayes -f reference.fasta SRR26779678.bam  -F 0.6 --ploidy 1 | bcftools view -Oz -o SRR26779678.vcf.gz\nbcftools index SRR26779678.vcf.gz\nawk '$3&lt;100' SRR26779678.depth.txt | awk '{print $1\"\\t\"$2-1\"\\t\"$2}' &gt; SRR26779678.low-dp.bed\nbcftools consensus -f reference.fasta -p \"SRR26779678 \" --mask SRR26779678.low-dp.bed -M N SRR26779678.vcf.gz  &gt; SRR26779678.consensus.fasta\n\ncat SRR26779702.consensus.fasta SRR26779678.consensus.fasta zika_dataset.fasta &gt; unaligned.fasta\nmafft unaligned.fasta &gt; zika_all_aligned.fasta\n\niqtree -s ./zika_all_aligned.fasta -bb 1000 -nt AUTO\n\nfigtree ./zika_all_aligned.fasta.treefile\n</code></pre>"},{"location":"tb-genomics/","title":"TB Genomics","text":""},{"location":"tb-genomics/#intended-learning-outcomes","title":"Intended learning outcomes","text":"<p>The underlying theme of this practical is the use of genomics in understanding TB drug resistance and strain types. The practical is split into three sections. Each using different programs and with a different data type(s). For each section try to:</p> <ul> <li>Be familiar with the programs presented and the types of analyses the can perform</li> <li>Understand the format of the input/output data</li> <li>Understand the conclusions you can draw from the analysis</li> <li>Think about how you can apply these concepts to different organisms/scenarios</li> </ul>"},{"location":"tb-genomics/#1-whole-genome-phylogenetic-analysis","title":"1. Whole genome phylogenetic analysis","text":"<p>In this section we will be generating phylogenetic trees from whole-genome polymorphisms. The dataset consists of 51 M. tuberculosis isolates, which underwent DNA sequencing using Illumina (Genome Analyser II, 76-bp paired-end) technology. Samples were isolated from 41 treatment-experienced TB patients attending a clinic in Kampala, Uganda, including longitudinal samples from five patients and cases of multi drug-resistant TB (MDR-TB). Raw reads were mapped to the H37Rv reference genome using BWA software and subsequently 6857 SNPs inferred employing SAMTOOLS and BCF/VCFTOOLS.  By concatenating the SNP locations for the 51 samples, a FASTA formatted dataset (uganda_gen.fasta) has been prepared to read into most phylogenetics software. Here we will be using seaview (Gouy, Guindon, &amp; Gascuel, 2010), a graphical user interface for multiple sequence alignment and molecular phylogeny. </p> <p>Open up a new seaview window (by typing <code>seaview</code> on the terminal). Select File -&gt; Open and choose \"uganda_gen.fasta\"</p> <p> Figure 1 Fasta-formatted sequence alignment of 51 Mtb isolates</p> <p>The alignment should appear in the seaview main window (Figure 1). The nucleotide bases are colour-coded to visually identify mismatches across sequences. The sequence identifiers on the left hand side of the window (e.g. A70659) consist of the patient ID, where multiple samples have been taken from the same patient the ID is suffixed with a number indicating which timepoint (e.g. A70136_1). A molecular typing method called spoligotyping has also been performed on most of the isolates. Box 1 explains the spoligotyping technique in Mycobacterium tuberculosis. The spoligotypes are in the <code>meta.csv</code> file.</p> <p>To launch the phylogenetic analysis go to Trees -&gt; Distance Methods. Select the Neighbor-joining method, Jukes-Cantor distance and Bootstrap with 100 replicates (see Figure 2)</p> <p> Figure 2 Distance analysis window</p> <p> Figure 3 Neighbour-joining phylogenetic tree of 51 Mtb isolates</p> <p>Look at the resulting distance-based tree (Figure 3). Select Br lengths and Bootstrap tick boxes to display branch lengths and bootstrap values respectively.</p> <p>Info</p> <p>Box 1 - Spoligotyping The popular spoligotyping approach (Kamerbeek et al., 1997) exploits the polymorphism at the direct repeat (DR) locus of Mycobacterium tuberculosis. It is based on the PCR amplification of 43 short unique sequences (termed spacers) found between well-conserved 36-bp DRs and the subsequent hybridisation of the products onto a membrane with oligonucleotides complementary to each spacer. Since strains vary in the occurrence of particular spacers, each sample produces a distinctive spot pattern, which is then translated into a numerical code of 15 digits (known as octal code). Each code or group of related codes correspond to particular strain types such as Beijing, Central-Asian (CAS) or Latin-American (LAM) types.</p> <p> Figure 4 Spoligotying scematic </p> <p>Question</p> <p>Compare whole-genome and spoligotype-based clustering. Which clustering method provides better resolution, i.e. enables a deeper isolate differentiation? In general, whole-genome SNP clustering provides greater resolution since isolates from the same spoligotype strain can be distinguished. How can these differences be explained?</p> <p>Isolates sequenced at different time points from the same patient are closely related and cluster together with very short branch lengths among them. This is the case of A70136, A70763 and A70144 patients pointed with green arrows in Figure 3. On the contrary, isolates from patient A70067 and A70011 (red arrows) happen to be placed at distant positions in the tree and having different spoligotypes. In which terms could this be explained? We will now explore this for patient A70067.</p> <p>You have seen that genome alignments can be used to construct phylogenetic trees to understand the population structure of samples. They tend to cluster by strain-type In the Kampalan set, but this also holds across a global set of samples. Spoligotyping is one way of defining strain-type, but M. tuberculosis sensu stricto can be classified into seven lineages, including four that are predominant; 1 Indo-Oceanic, 2 East-Asian including Beijing, 3 East-African-Indian, 4 Euro-American. These lineages are postulated to have differential roles in pathogenesis, disease outcome and variation in vaccine efficacy. For example, modern lineages, such as Beijing and Euro-American Haarlem strains exhibit more virulent phenotypes compared to ancient lineages, such as East African Indian (Coll et al, 2014).</p> <p>Before you close seaview, save the phylogenetic tree be selecting \u201cFile -&gt; Save rooted tree\u201d using <code>uganda_gen.nwk</code> as the file name.</p>"},{"location":"tb-genomics/#2-in-silico-strain-typing","title":"2. In-silico strain typing","text":"<p>Whole genome sequencing is making its way into a clinical setting to assist with patient management. Recent reports of sequencing M. tuberculosis from sputum from suspected drug resistant patients (Whitney et al, 2015) suggests it has a role in the management of tuberculosis. An exhaustive library of 1325 drug resistance markers has been established and integrated into an online tool that rapidly analyses raw sequence data and predicts resistance (Coll et al, 2015; Phelan et al 2019). The online tool is called \u201cTB-profiler\u201d and can be accessed via https://tbdr.lshtm.ac.uk/</p> <p>Click on upload and drag and drop in the A70067_1.fastq.gz file to the upload box, change the \u201cpairing\u201d option to \u201csingle\u201d and press \u201cSubmit\u201d. The output will take a few minutes to generate, as there are many millions of reads. Similarly, run it for the other sample from patient A70067 (A70067_2.fastq.gz). </p> <p> Figure 5 tb-profiler upload screen</p> <p>Info</p> <p></p> <p>If you are feeling extra impatient today you can find pre-computed results here:</p> <ul> <li>A70067_1 - https://tbdr.lshtm.ac.uk/results/0ba50688-d874-44e7-93c0-f78ec7334730</li> <li>A70067_2 - https://tbdr.lshtm.ac.uk/results/1dcefc81-70c1-40e4-8aa5-6ce03ee746bf</li> </ul> <p>Question</p> <p>How do the profiles differ from each other? Is this what you expected, especially when comparing to the original phylogenetic tree visualised in seaview? Are there any situations where this profiling approach could lead to an inconclusive result? </p>"},{"location":"tb-genomics/#3-genetic-variation-visualisation-between-different-strains","title":"3. Genetic variation visualisation between different strains","text":"<p>Here we use the Broad Institute's IGV genomic visualisation tool to consider SNP and structural variation differences between Kampalan Mtb alignments. </p> <p>Launch the IGV tool (typing \u201cigv\u201d on the command-line)</p> <p>The IGV application window is divided into several controls and panels (see figure below). If sample attributes are loaded, they will be colour-coded and displayed as columns on the left-hand side of its corresponding identifier.</p> <p> Figure 6 IGV overview panels and features</p> <p>We need to load in the reference genome and annotation into IGV. As you have done in previous practicals load the reference genome (MTB.fa) and annotation files (MTB.gff). </p> <p>You should now see the genome sequence in the top panel and the annotation in the bottom panel. Remember, the annotation file contains information about the location of genes, their function and other features of interest.</p> <p> Figure 7 IGV view of the reference genome and annotation</p> <p>We will be exploring the genomic data for samples A70067_1 (CAS2, lineage3, September 2003) and A70067_2 (T2, lineage 4, April 2004) which, as discussed in the previous exercise, were isolated from the same patient but are different strains being introduced at different times. We should now load in the BAM files (alignment files) from these two different strains in the previous loaded IGV environment</p> <p>Click on File &gt; Load from file... and select the corresponding BAM files (A70067_1.bam and A70067_2.bam). Go to the chromosome region harbouring the Rv3738c gene (coordinates 4,189,463 to 4,190,410 bp) using the Search Box. Type in \u2018Rv3738c\u2019 or \u2018Chromosome:4,189,463-4,190,410\u2019 in the search box. </p> <p>The IGV screen should now be focused on that gene, but try zooming out and in.</p> <p> Figure 8 Figure 4 IGV view showing a deletion at the Rv3738c locus in A70067_1 isolate</p> <p>Take a closer look at both the BAM and coverage tracks around Rv3738c gene. A deletion with respect to the reference can be spotted in A70067_1, whilst is absent in A70067_2. Several signatures are indicative of such event. The most obvious is the lack of read coverage at the region. Large deletions such as this one should also have paried reads that span the deletion junction. To view this, right click on the coverage track and select \"View as pairs\". This will show you the paired reads that are mapped to the reference genome. Can you see this evidence in the A70067_1 sample? What about in A70067_2?</p> <p>Samples belonging to CAS2 strains (e.g. A70067_1) are reported to have a deletion covering this region while other strains types are not. These regions are often called Regions of Difference (RD), and some RDs and SNPs are associated with strain-types (Coll et al, 2014). </p> <p>As shown earlier, Isolates A70067_1 and A70067_2 exhibit different drug susceptibility profiles (see Table 1). Both isolates were tested for Isoniazid (INH) and Rifampicin (RIF) anti-TB drugs, with A70067_1 being susceptible and A70067_2 multidrug-resistant. </p> Patient Date SIT Spoligotype family (lineage) Drug     INH RIF Compared      to SNPs      All DR A70067_1 Sep-03 288 CAS2 (lineage 3) S  S H37Rv 1060 (539) 15 (9) A70067_2 Apr-04 2867 T2 (lineage 4) R  R H37Rv 475 (246) 11 (9) <p>Drugs: INH = Isoniazid , RIF=  Rifampicin, R = resistant, S = susceptible; DR = drug resistant candidates. SNP (non-synonymous changes). </p> <p>The primary mechanism for acquiring resistance in M. tuberculosis is the accumulation of point mutations (SNPs) in genes coding for drug targets or converting enzymes, and drug resistant disease arises through selection of mutants during inadequate treatment (Zhang &amp; Vilcheze, 2005) or new transmitted resistant strains. We will investigate polymorphism differences between A70067_1 and A70067_2 isolates in katG (Rv1908c, coordinates: 2153896-2156118 bp) and rpoB (Rv0667, coordinates: 759810-763328 bp) genes, known to be associated with INH and RIF resistance respectively.</p> <p>Make use of the Search Box functionality again to go to the genes of interest. Try to spot differences between the susceptible and resistant isolates in terms of presence/absence of SNPs (Figure 9). Note that lineage-specific SNPs may also be present. Table 2 contains drug resistance and lineage-specific SNPs found in the rpoB and katG genes. Mismatches are colour-coded, while nucleotides matching the reference are not. Nevertheless, not all mismatches are to be considered SNPs since some differences are due to sequencing errors. On average, 1 in every 1000 bases in the reads is expected to be incorrect. However, the high depth of coverage achieved by current sequencing platforms means SNPs can be distinguished from sequencing errors. True SNPs are expected to be mismatches occurring consistently across multiple reads at the same reference position, whereas mismatches at spurious locations are likely to be caused by sequencing errors.</p> <p> Figure 9 IGV view focused on rpoB gene (locus tag: Rv0667, coordinates: 759810-763328 bp)</p> <p>Question</p> <p>Can you identify the mutations in the katG and rpoB genes that are associated with INH and RIF resistance respectively? How do these mutations differ between the two isolates? (Use the table below to help you).</p> Gene Locus Name Chromosome position Nucleotide change Amino acid change and codon number Annotation katG Rv1908c 2155168 C/A S315I INH resistance conferring SNP katG Rv1908c 2155168 C/T S315N INH resistance conferring SNP katG Rv1908c 2155168 C/G S315T INH resistance conferring SNP katG Rv1908c 2154724 C/A R463L Non-lineage 4 specific SNP rpoB Rv0667 761155 C/T S450L RIF resistance conferring SNP rpoB Rv0667 761155 C/G D450W RIF resistance conferring SNP rpoB Rv0667 762434 T/G G876G CAS/lineage 3 specific SNP rpoB Rv0667 763031 T/C A1075A Non-lineage 4 specific SNP"},{"location":"tb-genomics/#4-investigating-transmission","title":"4. Investigating transmission","text":"<p>Using web-based tools is a great way to run software without the need for installation or knowledge of Linux or the command-line. However, these are often not convenient to use if you have many samples, or don\u2019t have access to the internet. Many of the tools that we have used today are also available as command-line software (or have equivalents). To demonstrate this, we will run tb-profiler on our terminal. </p> <p>Before running tb-profiler we will need to activate the environment using the following command:</p> <pre><code>conda activate tb-profiler\n</code></pre> <p>After this we can run the profiling as we have done using tbdr.lshtm.ac.uk. The following commands will change directory to where the data is and profile the first sample:</p> <pre><code>cd ~/Data/tb\ntb-profiler profile --read1 A70067_1.fastq.gz --prefix A70067_1 --txt\n</code></pre> <p>There are a few arguments that we have given:</p> <ul> <li><code>--read1</code> : This allows us to specify the input fastq file</li> <li><code>--prefix</code> : This specifies the prefix for the output files</li> <li><code>--txt</code> : This ensures a text output will be created </li> </ul> <p>Question</p> <p>Have a look at the output (~/Data/tb/results/A70067_1.results.txt). Does it look the same as the profile we got from tbdr.lshtm.ac.uk? </p> <p>Try running the command for A70067_2 by changing the appropriate parameters.</p> <p>We can then combine several runs of tb-profiler into a single summary file with the collate function:</p> <pre><code>tb-profiler collate\n</code></pre> <p>Question</p> <p>Have a look at the output file ~/Data/tb/tbprofiler.txt using a text editor.  Do the profiles look different?</p> <p>Now we\u2019ll run tb-profiler on all samples. Running it from fastq files can take some time as it must go through all processing steps including trimming, mapping and variant calling. As a result it can take a while to run. Tb-profiler also can take input from vcf files which just contain variants. To run the pipeline using the provided vcfs the command should look like:</p> <pre><code>tb-profiler profile --vcf A70067_1.vcf.gz --prefix A70067_1 --txt\n</code></pre> <p>Here we have just switched the <code>--read1</code> argument for the <code>--vcf</code> argument. </p> <p>To establish if transmission has occurred, we frequently calculate snp-distance between pairs of samples and pairs with a distance less than a pre-defined cutoff (e.g. 10 snps) can be linked. We can investigate this by telling tb-profiler to calculate snp-distances and save the results of any pairs below a threshold. To do we can add <code>--snp_dist 20</code> to the command. We have used a relaxed value of 20 here as we can always make this more stringent after.</p> <p>Finally, it can be a bit repetitive to run the same command for many isolates. Ideally, we would like to harness the power of automation that is possible on the command-line so that we can profile all samples with just one command. Tb-profiler contains a batch mode that enables you set up your commands using a csv file that tells the pipeline where to look for the input files. To do this we\u2019ll create a csv file using some command-line tools:</p> <pre><code>ls *.vcf.gz | awk 'BEGIN {print \"id,vcf\"} {print $1\",\"$1}' | sed 's/.vcf.gz//' &gt; vcf_files.csv\n</code></pre> <p>This will create a csv file with two columns: id and vcf. Have a look at the format by running <code>head vcf_files.csv</code>. Now you can tell tb-profiler to run the pipeline for all samples in the csv file by running:</p> <pre><code>tb-profiler batch --csv vcf_files.csv --args \"--snp_dist 20\"\n</code></pre> <p>You\u2019ll notice we didn\u2019t give the <code>--vcf</code> and <code>--prefix</code> arguments as these are read from the csv file. We do have to specify any additional arguments with <code>--args</code> followed by the arguments as you would type them for a single sample command.</p> <p>After it has finished running we can again combine the output files into a single report by running:</p> <pre><code>tb-profiler collate --itol\n</code></pre> <p>This will produce in addition to the tbprofiler.txt output file it will also create a file called tbprofiler.transmission_graph.json. We will visualise this in a web-based tool that you can open in your web browser at https://jodyphelan.github.io/tgv</p> <p>After the page loads click on the upload box and select the .json file. This represents the samples as nodes and where pairs of samples have a snp-distance less or equal to the cutoff an edge will be drawn between them (Figure 10). </p> <p>Question</p> <p>Does this agree with the phylogenetic tree? What happens if you alter the snp-distance cutoff to be more conservative (e.g. 10 SNPs)? What do you think is the best cutoff to use?</p> <p> Figure 10 A transmission network generated by tb-profiler</p> <p>Finally, the collate command also produces annotation config files that can be used with iTOL which is a web-based phylogenetic tree viewer. To do this open https://itol.embl.de/ and select \u201cupload\u201d on the top navbar. Click browse and select the .nwk  file that you saved at the beginning of the practical, followed by \u201cupload\u201d. You should now see the tree as you have seen before in seaview. To load the annotations simple drag and drop the following files onto the tree view: -   Tbprofiler.lineage.itol.txt \u2013 a strip with the main lineage annotated. -   Tbprofiler.dr.itol.txt \u2013 a strip indicating the drug resistance type. -   Tbprofiler.dr.individ.itol.txt \u2013 circles for each individual drug where a solid filled circle indicates resistance.</p> <p> Figure 11 Annotations profuced by tb-profiler visualised with iTOL</p>"}]}